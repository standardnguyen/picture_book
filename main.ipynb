{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "from utils import write_text_to_file, number_to_four_digit_string\n",
    "from config import name_of_book, full_book_txt, client, model_to_use, backup_model\n",
    "\n",
    "count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.01\n"
     ]
    }
   ],
   "source": [
    "chunk_directory=os.path.join(name_of_book,\"text_chunks\")\n",
    "\n",
    "with open(full_book_txt, 'r', encoding='utf-8') as file:\n",
    "    remaining_text = full_text = file.read()\n",
    "\n",
    "if os.path.exists(chunk_directory):\n",
    "    files = [f for f in os.listdir(chunk_directory) if os.path.isfile(os.path.join(chunk_directory, f))]\n",
    "    # Sort files alphabetically\n",
    "    files.sort()\n",
    "    # Return the last file if there are any files\n",
    "    if files:\n",
    "        filepath = os.path.join(chunk_directory, files[-1])\n",
    "\n",
    "        name_of_file = files[-1]\n",
    "        without_the_other_stuff = int(name_of_file.replace(name_of_book,\"\").replace(\".txt\",\"\"))\n",
    "        count += without_the_other_stuff\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            finished_text = file.read()\n",
    "\n",
    "            length_to_cut = remaining_text.find(finished_text)\n",
    "            length_to_cut += len(finished_text)\n",
    "\n",
    "            remaining_text=remaining_text[length_to_cut:]\n",
    "\n",
    "# print(remaining_text)\n",
    "percent_complete = 100 - (len(remaining_text)/len(full_text))*100\n",
    "percent_complete = round(percent_complete, 2)\n",
    "print(percent_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Literature Chunker\",\n",
    "    instructions=\"You are meant to divide text into segments based on natural breaks in the narrative, ensuring chunks can stand alone with its accompanying illustration\",\n",
    "    tools=[],\n",
    "    model=model_to_use,\n",
    ")\n",
    "\n",
    "assistant2 = client.beta.assistants.create(\n",
    "    name=\"Literature Chunker\",\n",
    "    instructions=\"You are meant to divide text into segments based on natural breaks in the narrative, ensuring chunks can stand alone with its accompanying illustration\",\n",
    "    tools=[],\n",
    "    model=backup_model,\n",
    ")\n",
    "\n",
    "def limit_context_window(upgrade=False):\n",
    "\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    prompt = \"You are meant to divide text into segments based on natural breaks in the narrative, ensuring chunks can stand alone with its accompanying illustration. Do not give a break that causes a sentence to be split in half. A few examples are: \"\n",
    "    prompt += '''\\n\"No, it mustn't be so any more; we must not part like that! what are two evenings?\"'''\n",
    "    prompt += '\\n\"I bless my luck for the excellent knotted stick, which happened on that occasion to be in my right hand.\"'\n",
    "    prompt += '\\n\"And timid as I was with women, yet this was such a moment!\"'\n",
    "\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=prompt\n",
    "    )\n",
    "\n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id if upgrade == False else assistant.id,\n",
    "        instructions=\"Please follow the user's instructions and acknowlege that you understand.\",\n",
    "        # event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        limit=1\n",
    "\n",
    "    )\n",
    "\n",
    "    return thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_to_cut(literature, count=0):\n",
    "    prompt = \"Okay, find the first natural break in this snippet, where you think it would be appropriate to add an accompanying illustration.\"\n",
    "    prompt += \"The intention is that the chunks can stand alone with its accompanying illustration.\"\n",
    "    prompt += \"Tell me where the break is by telling me the last ten or so words from the snippet that marks the break.\"\n",
    "    prompt += \"**Enclose the last sentence** between two triple hashes, like ###At this point Nastenka stopped and began laughing.###\"\n",
    "    # prompt += \"The last few bits that marks the break, should have the exact punctuation from the text.\"\n",
    "    prompt += \"**Make sure that you do not cut off a word or a sentence**.\"\n",
    "    prompt += \"Do not capitalize anything unless it was already capitalized. Do not lowercase anything unless it was already lowercased.\"\n",
    "    prompt += \"Do not give me multiple breaks.\"\n",
    "    prompt += \"Take all the time you need to get it done right the first time. It's better to be late than incorrect.\"\n",
    "    prompt += \"Please, this is for spreading literature to the masses.\"\n",
    "\n",
    "    if count > 20: prompt += f\"This is not the first time I have asked variants of you for it, and it's still not working. Please look for the {round(count/10) + 1}th best solution\"\n",
    "\n",
    "    thread = limit_context_window(upgrade=False if count < 10 else True)\n",
    "\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=prompt+\"'''\" + literature + \"'''\"\n",
    "    )\n",
    "\n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=\"Please follow the user's instructions.\",\n",
    "        # event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        limit=1\n",
    "\n",
    "    )\n",
    "    # type(messages)\n",
    "    length_to_cut = 0\n",
    "    for message in messages:\n",
    "        raw_response = message.content[0].text.value\n",
    "\n",
    "        pattern = r'###(.*?)###'\n",
    "\n",
    "        # Use re.search to grab the first occurrence\n",
    "        match = re.search(pattern, raw_response)\n",
    "\n",
    "        if not match:\n",
    "            if count % 40 == 0 and count != 0: print(\"\\n\")\n",
    "            print(\"🙅\",end=\"\")\n",
    "            return get_length_to_cut(literature, count=count+1)\n",
    "        text_that_got_returned = match.group(1)\n",
    "\n",
    "        if len(text_that_got_returned) < 30:\n",
    "            if count % 40 == 0 and count != 0: print(\"\\n\")\n",
    "            print(\"🤏\",end=\"\") \n",
    "            return get_length_to_cut(literature, count=count+1)\n",
    "        if text_that_got_returned[-1] not in [\".\",\"!\",\"?\",'\"']:\n",
    "            if count % 40 == 0 and count != 0: print(\"\\n\")\n",
    "            print(\"🔚\",end=\"\")\n",
    "            return get_length_to_cut(literature, count=count+1)\n",
    "        if text_that_got_returned[-1] == text_that_got_returned[-2] == \".\":\n",
    "            if count % 40 == 0 and count != 0: print(\"\\n\")\n",
    "            print(\"😶\",end=\"\")\n",
    "            return get_length_to_cut(literature, count=count+1)\n",
    "\n",
    "\n",
    "        # print(text_that_got_returned)\n",
    "        length_to_cut = literature.find(text_that_got_returned)\n",
    "        if length_to_cut == -1:\n",
    "            if count % 40 == 0 and count != 0: print(\"\\n\")        \n",
    "            print(\"😱\",end=\"\")\n",
    "            return get_length_to_cut(literature, count=count+1)\n",
    "\n",
    "        \n",
    "        print(text_that_got_returned)\n",
    "        length_to_cut += len(text_that_got_returned)\n",
    "        break    \n",
    "\n",
    "    return length_to_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔚🔚It seemed to me that calculation was superfluous, and by no means possessed of the importance which certain other players attached to it, even though they sat with ruled papers in their hands, whereon they set down the coups, calculated the chances, reckoned, staked, and—lost exactly as we more simple mortals did who played without any reckoning at all.\n",
      "Text has been written to thegambler/text_chunks/0055.txt\n",
      "### Percent Complete: 14.13%\n",
      "😱😱🔚🔚😱😱😱😱😱🔚🤏🤏🙅🤏whence, after another couple of rounds, the ball would again pass to the first figures, strike upon them once, and then return thrice to the middle series—continuing thus for an hour and a half, or two hours.\n",
      "Text has been written to thegambler/text_chunks/0056.txt\n",
      "### Percent Complete: 14.36%\n",
      "😱Of these circumstances the majority were pointed out to me by Mr. Astley, who stood by the gaming-table the whole morning, yet never once staked in person.\n",
      "Text has been written to thegambler/text_chunks/0057.txt\n",
      "### Percent Complete: 14.55%\n",
      "😱🙅😱😱🤏🤏😱🤏😱😱🤏🙅😱😱🔚😱🤏😱😱😱🙅😱🙅🙅🙅🤏🤏🤏🤏🤏😱😱🔚🙅🔚🔚😱and replied that the money had been all my own.\n",
      "Text has been written to thegambler/text_chunks/0058.txt\n",
      "### Percent Complete: 14.99%\n",
      "🤏🔚🤏At this the General seemed extremely surprised, and asked me whence I had procured it; whereupon I replied that, though I had begun only with 100 gülden, six or seven rounds had increased my capital to 5000 or 6000 gülden, and that subsequently I had lost the whole in two rounds.\n",
      "Text has been written to thegambler/text_chunks/0059.txt\n",
      "### Percent Complete: 15.08%\n",
      "🔚🔚🔚🤏🔚😱🔚🙅🤏🙅😱🔚🔚🤏😱🔚🤏🔚😱🔚🔚😱😱🤏🤏🤏🔚🤏😱🔚🔚😱😱😱🔚🤏😱🙅🤏😱\n",
      "\n",
      "🔚😱🤏🔚🤏🔚At all events they closeted themselves together, and then had a long and vehement discussion; after which the Frenchman departed in what appeared to be a passion, but returned, early this morning, to renew the combat.\n",
      "Text has been written to thegambler/text_chunks/0060.txt\n",
      "### Percent Complete: 15.41%\n",
      "😱😱🤏“Oh no, it is not true,” put in the General sternly.\n",
      "Text has been written to thegambler/text_chunks/0061.txt\n",
      "### Percent Complete: 15.84%\n",
      "😱😱😱😱😱😱🤏🔚😱🙅😱🙅😱😱😱😱😱🙅🙅🙅😱it is dreadful to have anything to do with him; and I cannot bear people of that sort.\n",
      "Text has been written to thegambler/text_chunks/0062.txt\n",
      "### Percent Complete: 16.18%\n",
      "Yes, such things ARE done, for I have been making inquiries on the subject. It is all done out of sheer rectitude—out of a rectitude which is magnified to the point of the younger son believing that he has been rightly sold, and that it is simply idyllic for the victim to rejoice when he is made over into pledge.\n",
      "Text has been written to thegambler/text_chunks/0063.txt\n",
      "### Percent Complete: 16.61%\n",
      "Gretchen’s cheeks grow sunken, and she begins to wither; until at last, after some twenty years, their substance has multiplied, and sufficient gülden have been honourably and virtuously accumulated.\n",
      "Text has been written to thegambler/text_chunks/0064.txt\n",
      "### Percent Complete: 16.78%\n",
      "🤏I have no wish to be ‘Hoppe and Company’ at the end of five generations.\n",
      "Text has been written to thegambler/text_chunks/0065.txt\n",
      "### Percent Complete: 17.17%\n",
      "🤏It was the first time that I had heard her speak so of De Griers: consequently, I was momentarily awed into silence by this expression of resentment.\n",
      "Text has been written to thegambler/text_chunks/0066.txt\n",
      "### Percent Complete: 17.62%\n",
      "🙅🙅🤏🔚🔚🔚🤏😱🔚🔚At his age it is a dangerous thing to fall in love.\n",
      "Text has been written to thegambler/text_chunks/0067.txt\n",
      "### Percent Complete: 17.89%\n",
      "😱🔚🔚😱🤏😱🔚🙅😱🔚Consequently, I am the more surprised that you should be so cheerful.\n",
      "Text has been written to thegambler/text_chunks/0068.txt\n",
      "### Percent Complete: 18.09%\n",
      "🤏🔚🤏till, if you do wish to know, I am in debt.\n",
      "Text has been written to thegambler/text_chunks/0069.txt\n",
      "### Percent Complete: 18.44%\n",
      "🤏🤏🙅🔚😱😱🤏🙅😱🔚🤏😱😱🔚🤏😱🔚😱🔚🔚😱😱🙅🤏🔚Perhaps because one cannot help winning if one is fanatically certain of doing so.\n",
      "Text has been written to thegambler/text_chunks/0070.txt\n",
      "### Percent Complete: 18.88%\n",
      "🔚🙅🤏🙅🤏🔚😱🤏🙅😱🤏🔚🙅🤏🤏🤏🙅🔚🤏🙅🔚🔚🙅😱🔚😱🤏🔚🔚🔚🔚🔚You talk like a child. It is always possible to comport oneself with dignity. If one has a quarrel it ought to elevate rather than to degrade one.\n",
      "Text has been written to thegambler/text_chunks/0071.txt\n",
      "### Percent Complete: 19.19%\n",
      "🤏The Frenchman is only a bird—the coq gaulois. At the same time, as I am not a woman, I do not properly understand the question. Cocks may be excellent birds.\n",
      "Text has been written to thegambler/text_chunks/0072.txt\n",
      "### Percent Complete: 19.6%\n",
      "🙅🔚😱🤏🔚the argument. If you do not wish to purchase me, at all events you wish to purchase my respect.\n",
      "Text has been written to thegambler/text_chunks/0073.txt\n",
      "### Percent Complete: 20.0%\n",
      "🙅🤏🙅🙅🙅😱🙅🙅🙅🙅🙅🙅🙅What rubbish!” she cried with a shudder.\n",
      "Text has been written to thegambler/text_chunks/0074.txt\n",
      "### Percent Complete: 20.34%\n",
      "hat would have been of no use to me.\n",
      "Text has been written to thegambler/text_chunks/0075.txt\n",
      "### Percent Complete: 20.77%\n",
      "🙅🤏🙅🔚😱🙅🤏🤏🔚🤏🙅🙅🤏😱🤏🙅🙅🔚😱🙅🙅🤏🙅🙅🙅😱🤏😱Yes, had she bidden me in jest, or only in contempt and with a spit in my face, I should have cast myself down.\n",
      "Text has been written to thegambler/text_chunks/0076.txt\n",
      "### Percent Complete: 21.06%\n",
      "😱“Well, then—I will kill whomsoever you wish,” I said.\n",
      "Text has been written to thegambler/text_chunks/0077.txt\n",
      "### Percent Complete: 21.51%\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_9a3f94dff0213e739448c62b68b703fd in your email.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m literature \u001b[38;5;241m=\u001b[39m remaining_text[:window_size]\n\u001b[1;32m      6\u001b[0m remaining_text \u001b[38;5;241m=\u001b[39m remaining_text[window_size:]\n\u001b[0;32m----> 8\u001b[0m length_to_cut \u001b[38;5;241m=\u001b[39m \u001b[43mget_length_to_cut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliterature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m chunk \u001b[38;5;241m=\u001b[39m literature[:length_to_cut]\n\u001b[1;32m     11\u001b[0m remainder \u001b[38;5;241m=\u001b[39m literature[length_to_cut:]\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mget_length_to_cut\u001b[0;34m(literature, count)\u001b[0m\n\u001b[1;32m     11\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, this is for spreading literature to the masses.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m20\u001b[39m: prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is not the first time I have asked variants of you for it, and it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms still not working. Please look for the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(count\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mth best solution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m thread \u001b[38;5;241m=\u001b[39m \u001b[43mlimit_context_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupgrade\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m message \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     18\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     19\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     content\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m literature \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     24\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     25\u001b[0m     assistant_id\u001b[38;5;241m=\u001b[39massistant\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     26\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease follow the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms instructions.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# event_handler=EventHandler(),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mlimit_context_window\u001b[0;34m(upgrade)\u001b[0m\n\u001b[1;32m     24\u001b[0m message \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     25\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     26\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     content\u001b[38;5;241m=\u001b[39mprompt\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     31\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     32\u001b[0m     assistant_id\u001b[38;5;241m=\u001b[39massistant\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mif\u001b[39;00m upgrade \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m assistant\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     33\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease follow the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms instructions and acknowlege that you understand.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# event_handler=EventHandler(),\u001b[39;00m\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m messages \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     39\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     40\u001b[0m     limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m thread\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py:102\u001b[0m, in \u001b[0;36mAssistantEventHandler.until_done\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muntil_done\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Waits until the stream has been consumed\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mconsume_sync_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/_utils/_streams.py:6\u001b[0m, in \u001b[0;36mconsume_sync_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconsume_sync_iterator\u001b[39m(iterator: Iterator[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py:69\u001b[0m, in \u001b[0;36mAssistantEventHandler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[AssistantStreamEvent]:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/lib/streaming/_assistants.py:404\u001b[0m, in \u001b[0;36mAssistantEventHandler.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStream has not been started yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emit_sse_event(event)\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/_streaming.py:46\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_T]:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/picture_book-oYbvcmtK/lib/python3.10/site-packages/openai/_streaming.py:91\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     89\u001b[0m                 message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[1;32m     92\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m     93\u001b[0m                 request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m     94\u001b[0m                 body\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sse\u001b[38;5;241m.\u001b[39mevent}, cast_to\u001b[38;5;241m=\u001b[39mcast_to, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "\u001b[0;31mAPIError\u001b[0m: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_9a3f94dff0213e739448c62b68b703fd in your email.)"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "window_size = 1500\n",
    "while len(remaining_text) > 0:\n",
    "\n",
    "    literature = remaining_text[:window_size]\n",
    "    remaining_text = remaining_text[window_size:]\n",
    "    \n",
    "    try:\n",
    "        length_to_cut = get_length_to_cut(literature)\n",
    "    except openai.error.APIError as e:\n",
    "        length_to_cut = get_length_to_cut(literature, count=0)\n",
    "    \n",
    "    chunk = literature[:length_to_cut]\n",
    "    remainder = literature[length_to_cut:]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "    remaining_text = remainder + remaining_text\n",
    "\n",
    "    write_text_to_file(chunk_directory, number_to_four_digit_string(count), literature[:length_to_cut])\n",
    "    count+=1\n",
    "    # we're going to print the percent complete, to two decimal places\n",
    "    percent_complete = 100 - (len(remaining_text)/len(full_text))*100\n",
    "    percent_complete = round(percent_complete, 2)\n",
    "    print(f\"### Percent Complete: {percent_complete}%\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picture_book-oYbvcmtK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
